{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f66961",
   "metadata": {},
   "source": [
    "# Text classification: classify spam email and non-spam email by several machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb69c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "import pmdarima\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "from pandas import MultiIndex, Int16Dtype\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import openpyxl\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from textwrap import wrap\n",
    "import inspect\n",
    "import itertools\n",
    "\n",
    "import inspect\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import xlsxwriter\n",
    "import xlwings as xw\n",
    "from copy import copy\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy.optimize import curve_fit\n",
    "import time\n",
    "from IPython import display as ICD\n",
    "import warnings\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl.styles import PatternFill \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaec7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import callbacks\n",
    "from keras.layers import Dropout\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from nltk.classify.util import accuracy\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14f7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='C:\\\\Users\\\\lairx78\\\\Desktop\\\\python'\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dfd813b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ham\n",
       "1        ham\n",
       "2       spam\n",
       "3        ham\n",
       "4        ham\n",
       "        ... \n",
       "5567    spam\n",
       "5568     ham\n",
       "5569     ham\n",
       "5570     ham\n",
       "5571     ham\n",
       "Name: v1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataframe\n",
    "\n",
    "df = pd.read_csv('Spam-Classification.csv')\n",
    "df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56396b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "\n",
    "df_ham_undersampling=df[df.iloc[:,0]=='ham'].sample(n = len(df[df.iloc[:,0]=='spam']), random_state = 123)\n",
    "df_spam=df[df.iloc[:,0]=='spam']\n",
    "df = pd.concat([df_ham_undersampling, df_spam])\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fc3763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ha ha - had popped down to the loo when you he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ever green quote ever told by Jerry in cartoon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rats. Hey did u ever vote for the next themes?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry I flaked last night, shit's seriously go...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>spam</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>spam</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Ha ha - had popped down to the loo when you he...        NaN   \n",
       "1      ham  Ever green quote ever told by Jerry in cartoon...        NaN   \n",
       "2      ham     Rats. Hey did u ever vote for the next themes?        NaN   \n",
       "3      ham  Sorry I flaked last night, shit's seriously go...        NaN   \n",
       "4      ham      Thank you. do you generally date the brothas?        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "1489  spam  Want explicit SEX in 30 secs? Ring 02073162414...        NaN   \n",
       "1490  spam  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...        NaN   \n",
       "1491  spam  Had your contract mobile 11 Mnths? Latest Moto...        NaN   \n",
       "1492  spam  REMINDER FROM O2: To get 2.50 pounds free call...        NaN   \n",
       "1493  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "1489        NaN        NaN  \n",
       "1490        NaN        NaN  \n",
       "1491        NaN        NaN  \n",
       "1492        NaN        NaN  \n",
       "1493        NaN        NaN  \n",
       "\n",
       "[1494 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85336bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding\n",
    "\n",
    "label = LabelEncoder().fit_transform(df.iloc[:,0])\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788bab7",
   "metadata": {},
   "source": [
    "# Data cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688f672",
   "metadata": {},
   "source": [
    "Remove or replace meaningless data using regular expressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1cda3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "message=df.iloc[:,1].str.lower().copy()\n",
    "\n",
    "message=message.str.replace(r'[^ ]+@[^\\.]*(\\.[a-z]{2,}){1,2}', 'emailaddress', regex=True)\n",
    "message=message.str.replace(r'(?:http\\:\\/\\/|www.){1}(?:http\\:\\/\\/|www.)?(?![www])[a-zA-Z0-9\\-]+(\\.[a-zA-Z]{2,3}){1,2}(\\/[^/& ]+)*', 'webaddress', regex=True)\n",
    "message=message.str.replace(r'(£|\\$|€|¥|₣|å£){1}\\d+(.\\d+)?', 'money', regex=True)\n",
    "message=message.str.replace(r'\\b\\+?\\d{1}[\\d\\s-]{5,13}\\d{1}\\b|\\b[\\d]{4}[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}\\b', 'phonenumber', regex=True)\n",
    "\n",
    "# date\n",
    "# day/month/year\n",
    "message=message.str.replace(r'\\b(3[01]|[12][0-9]|[0]?[1-9]){1}[\\ |\\-|\\/]{1}(1[0-2]|[0]?[1-9]){1}[\\ |\\-|\\/]{1}(\\d{2}|\\d{4})(?!\\d{3})\\b', 'date', regex=True)\n",
    "# year/month/day\n",
    "message=message.str.replace(r'\\b(\\d{2}|\\d{4})(?!\\d{3})[\\ |\\-|\\/]{1}(1[0-2]|[0]?[1-9]){1}[\\ |\\-|\\/]{1}(3[01]|[12][0-9]|[0]?[1-9]){1}\\b', 'date', regex=True)\n",
    "# day/month(english)/year\n",
    "message=message.str.replace(r'\\b(3[01]|[12][0-9]|[0]?[1-9]){1}([\\ ]|st|nd|rd|th){1,2}(jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?){1}\\b(?:([\\ ]|,){0,2}(\\d{4}|\\d{2}))?\\b', 'date', regex=True)\n",
    "# month(english)/day/year\n",
    "message=message.str.replace(r'\\b(jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?){1}[\\ ]?(3[01]|[12][0-9]|[0]?[1-9]){1}((?: )?st|(?: )?nd|(?: )?rd|(?: )?th){0,1}(?:([\\ ]|,){0,2}(\\d{4}|\\d{2}))?\\b', 'date', regex=True)\n",
    "\n",
    "\n",
    "message=message.str.replace(r'\\b\\d+\\b', 'number', regex=True)\n",
    "message=message.str.replace(r'[^\\w\\d\\s]+', ' ', regex=True)\n",
    "message=message.str.replace(r'\\s+', ' ', regex=True)\n",
    "message=message.str.replace(r'^\\s+|\\s+?$', '', regex=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f5655",
   "metadata": {},
   "source": [
    "Remove remove common words like 'a', 'the' using stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb0d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words=set(stopwords.words('english'))\n",
    "\n",
    "message=message.apply(lambda m: ' '.join(word for word in m.split() if word not in common_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c940f318",
   "metadata": {},
   "source": [
    "Change the form of words like 'running' to 'run' using Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3ec146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pos(tag):\n",
    "    if tag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wordnet.ADJ\n",
    "    elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wordnet.VERB\n",
    "    elif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wordnet.NOUN\n",
    "    elif tag in ['RB', 'RBR', 'RBS']:\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def convert_pos_2(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c39af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(message)):\n",
    "    tokens = word_tokenize(message[i])\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    tokens_list_tokenized = []\n",
    "    \n",
    "    for tag in tagged:\n",
    "        wordnet_pos = convert_pos(tag[1])\n",
    "        tokens_list_tokenized.append(WordNetLemmatizer().lemmatize(tag[0], pos=wordnet_pos))\n",
    "    \n",
    "    tokens_to_string = ' '.join(tokens_list_tokenized)\n",
    "    message[i]=tokens_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30042206",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail before cleaning:  ever green quote ever told by jerry in cartoon \\a person who irritates u always is the one who loves u vry much but fails to express it...!..!! :-) :-) gud nyt\" \n",
      "\n",
      "Mail after cleaning:  ever green quote ever tell jerry cartoon person irritate u always one love u vry much fails express gud nyt\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "print('Mail before cleaning: ', df.iloc[i,1].lower(), '\\n')\n",
    "print('Mail after cleaning: ', message[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9578fac",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea7b14",
   "metadata": {},
   "source": [
    "Tokenized all the words and choose the 500 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c21421e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Token: 3401\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "for m in message:\n",
    "    words = word_tokenize(m)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print('Total number of Token:', len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092a09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the 500 most common words\n",
    "word_most_common = [x[0] for x in all_words.most_common(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47a7b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_most_common(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_most_common:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32eb27",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731ba77",
   "metadata": {},
   "source": [
    "There are totally 7 machine learning models, incluing Decision Tree, Random Forest, Logistic Regression, SGD Classifier, Naive Bayes, Support Vector Classifier and Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b093b9",
   "metadata": {},
   "source": [
    "Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fd71872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data for training models:  1195 \n",
      "\n",
      "The number of data for testing models:  299\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "\n",
    "df_message = list(zip(message, label))\n",
    "np.random.shuffle(df_message)\n",
    "df_message_only_most_common = [(only_most_common(text), label) for (text, label) in df_message]\n",
    "training, test = train_test_split(df_message_only_most_common, test_size=0.2, random_state=1)\n",
    "\n",
    "print('The number of data for training models: ', len(training), '\\n')\n",
    "print('The number of data for testing models: ', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7733bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model Accuracy: 0.939799331103679\n",
      "Random Forest model Accuracy: 0.9565217391304348\n",
      "Logistic Regression model Accuracy: 0.9665551839464883\n",
      "SGD Classifier model Accuracy: 0.9565217391304348\n",
      "Naive Bayes model Accuracy: 0.9297658862876255\n",
      "Support Vector Classifier model Accuracy: 0.9698996655518395\n",
      "Gradient Boosting Classifier model Accuracy: 0.9665551839464883\n"
     ]
    }
   ],
   "source": [
    "names = ['Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD Classifier',\n",
    "         'Naive Bayes', 'Support Vector Classifier','Gradient Boosting Classifier']\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter=100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel='linear'),\n",
    "    GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    model_accuracy = accuracy(nltk_model, test)\n",
    "    print(\"{} model Accuracy: {}\".format(name, model_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455c0b4",
   "metadata": {},
   "source": [
    "Method 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85520c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data for training models:  1195 \n",
      "\n",
      "The number of data for testing models:  299\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "df_message_2 = list(zip(message, label))\n",
    "\n",
    "# shuffle\n",
    "np.random.shuffle(df_message_2)\n",
    "\n",
    "message_only_most_common = [only_most_common(text[0]) for text in df_message_2]\n",
    "list_label = [text[1] for text in df_message_2]\n",
    "\n",
    "x_training, x_test, y_training, y_test= train_test_split(message_only_most_common, list_label, test_size=0.2, random_state=1)\n",
    "\n",
    "print('The number of data for training models: ', len(x_training), '\\n')\n",
    "print('The number of data for testing models: ', len(x_test))\n",
    "\n",
    "x_training = DictVectorizer().fit_transform(x_training)\n",
    "x_test = DictVectorizer().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6c7cb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model Accuracy: 0.9297658862876255\n",
      "Random Forest model Accuracy: 0.9632107023411371\n",
      "Logistic Regression model Accuracy: 0.9698996655518395\n",
      "SGD Classifier model Accuracy: 0.9698996655518395\n",
      "Naive Bayes model Accuracy: 0.9163879598662207\n",
      "Support Vector Classifier model Accuracy: 0.9732441471571907\n",
      "Gradient Boosting Classifier model Accuracy: 0.9698996655518395\n"
     ]
    }
   ],
   "source": [
    "names = ['Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD Classifier',\n",
    "         'Naive Bayes', 'Support Vector Classifier','Gradient Boosting Classifier']\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter=100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel='linear'),\n",
    "    GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    nltk_model = classifiers[i]\n",
    "    nltk_model.fit(x_training, y_training)\n",
    "    model_accuracy = nltk_model.score(x_test, y_test)\n",
    "    print(\"{} model Accuracy: {}\".format(names[i], model_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3988b58",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0adeae",
   "metadata": {},
   "source": [
    "Volting the best prediction by ensemble all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64a63769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label: \n",
      " [0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0\n",
      " 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0\n",
      " 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1\n",
      " 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1\n",
      " 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1\n",
      " 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
      " 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1] \n",
      "\n",
      "Voting Classifier model Accuracy: 0.9732441471571907\n"
     ]
    }
   ],
   "source": [
    "names = ['Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD Classifier',\n",
    "         'Naive Bayes', 'Support Vector Classifier','Gradient Boosting Classifier']\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter=100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel='linear'),\n",
    "    GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "ensemble_models =VotingClassifier(estimators=models, voting='hard', n_jobs=1)\n",
    "ensemble_models.fit(x_training, y_training)\n",
    "model_accuracy = ensemble_models.score(x_test, y_test)\n",
    "\n",
    "print('The predicted label: \\n', ensemble_models.predict(x_test), '\\n')\n",
    "print(\"Voting Classifier model Accuracy: {}\".format(model_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a1a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
